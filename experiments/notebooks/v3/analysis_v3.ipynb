{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "015118e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-21 06:43:11.343907: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-21 06:43:11.520049: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-01-21 06:43:12.226133: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/modaresi/.conda/envs/globenc-venv/lib/\n",
      "2023-01-21 06:43:12.226245: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/modaresi/.conda/envs/globenc-venv/lib/\n",
      "2023-01-21 06:43:12.226252: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<contextlib._GeneratorContextManager at 0x7f09eab4d950>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import datasets\n",
    "import pickle\n",
    "import pathlib\n",
    "import os\n",
    "from importlib import reload\n",
    "import sys\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "import json\n",
    "import sklearn\n",
    "from sklearn.metrics import accuracy_score\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "import logging\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import DataCollatorWithPadding\n",
    "from transformers.trainer_pt_utils import LengthGroupedSampler\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "\n",
    "# For Imports\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "# print(\"sys.path:\", sys.path)\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.style.context(\"ggplot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58142b2b",
   "metadata": {},
   "source": [
    "# Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed0cca09",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS_DIR = \"/home/modaresi/projects/globenc_analysis/outputs/models\"\n",
    "OUTPUT_DIR = \"/home/modaresi/projects/globenc_analysis/outputs/globencs_v3\"\n",
    "IG_OUTPUT_DIR = \"/home/modaresi/projects/globenc_analysis/outputs/integrated_saliencies\"\n",
    "IXG_OUTPUT_DIR = \"/home/modaresi/projects/globenc_analysis/outputs/saliencies\"\n",
    "ALTI_OUTPUT_DIR = \"/home/modaresi/projects/globenc_analysis/outputs/ALTI\"\n",
    "\n",
    "DIR_MODEL_DATASET_SET = [\n",
    "    (OUTPUT_DIR, f\"{MODELS_DIR}/output_sst2_bert-base-uncased_0001_SEED0042/checkpoint-10525\", \"sst2\", \"validation\"),\n",
    "#     (OUTPUT_DIR, f\"{MODELS_DIR}/output_hatexplain_bert-base-uncased_0001_SEED0042/checkpoint-2405\", \"hatexplain\", \"validation\"),\n",
    "#     (OUTPUT_DIR, f\"{MODELS_DIR}/output_qnli_bert-base-uncased_0001_SEED0042/checkpoint-16370\", \"qnli\", \"validation\"),\n",
    "#     (OUTPUT_DIR, f\"{MODELS_DIR}/output_mnli_bert-base-uncased_0001_SEED0042/checkpoint-61360\", \"mnli\", \"validation_matched\"),\n",
    "    \n",
    "#     (f\"{MODELS_DIR}/output_cola_bert-base-uncased_0001_SEED0042/checkpoint-1340\", \"cola\", \"validation\"),\n",
    "#     (f\"{MODELS_DIR}/output_mrpc_bert-base-uncased_0001_SEED0042/checkpoint-575\", \"mrpc\", \"validation\"),\n",
    "#     (f\"{MODELS_DIR}/output_sst2_bert-large-uncased_0001_SEED0042/checkpoint-10525\", \"sst2\", \"validation\"),\n",
    "### IG ###\n",
    "    (IG_OUTPUT_DIR, f\"{MODELS_DIR}/output_sst2_bert-base-uncased_0001_SEED0042/checkpoint-10525\", \"sst2\", \"validation\"),\n",
    "#     (IG_OUTPUT_DIR, f\"{MODELS_DIR}/output_hatexplain_bert-base-uncased_0001_SEED0042/checkpoint-2405\", \"hatexplain\", \"validation\"),\n",
    "#     (IG_OUTPUT_DIR, f\"{MODELS_DIR}/output_qnli_bert-base-uncased_0001_SEED0042/checkpoint-16370\", \"qnli\", \"validation\"),\n",
    "#     (IG_OUTPUT_DIR, f\"{MODELS_DIR}/output_mnli_bert-base-uncased_0001_SEED0042/checkpoint-61360\", \"mnli\", \"validation_matched\"),\n",
    "### IXG ###\n",
    "    (IXG_OUTPUT_DIR, f\"{MODELS_DIR}/output_sst2_bert-base-uncased_0001_SEED0042/checkpoint-10525\", \"sst2\", \"validation\"),\n",
    "#     (IXG_OUTPUT_DIR, f\"{MODELS_DIR}/output_hatexplain_bert-base-uncased_0001_SEED0042/checkpoint-2405\", \"hatexplain\", \"validation\"),\n",
    "#     (IXG_OUTPUT_DIR, f\"{MODELS_DIR}/output_qnli_bert-base-uncased_0001_SEED0042/checkpoint-16370\", \"qnli\", \"validation\"),\n",
    "#     (IXG_OUTPUT_DIR, f\"{MODELS_DIR}/output_mnli_bert-base-uncased_0001_SEED0042/checkpoint-61360\", \"mnli\", \"validation_matched\"),\n",
    "### ALTI ###\n",
    "    (ALTI_OUTPUT_DIR, f\"{MODELS_DIR}/output_sst2_bert-base-uncased_0001_SEED0042/checkpoint-10525\", \"sst2\", \"validation\"),\n",
    "#     (ALTI_OUTPUT_DIR, f\"{MODELS_DIR}/output_hatexplain_bert-base-uncased_0001_SEED0042/checkpoint-2405\", \"hatexplain\", \"validation\"),\n",
    "#     (ALTI_OUTPUT_DIR, f\"{MODELS_DIR}/output_qnli_bert-base-uncased_0001_SEED0042/checkpoint-16370\", \"qnli\", \"validation\"),\n",
    "#     (ALTI_OUTPUT_DIR, f\"{MODELS_DIR}/output_mnli_bert-base-uncased_0001_SEED0042/checkpoint-61360\", \"mnli\", \"validation_matched\"),\n",
    "### RoBERTa ###\n",
    "#     (OUTPUT_DIR, f\"WillHeld/roberta-base-sst2\", \"sst2\", \"validation\"),\n",
    "#     (ALTI_OUTPUT_DIR, f\"WillHeld/roberta-base-sst2\", \"sst2\", \"validation\"),\n",
    "#     (OUTPUT_DIR, f\"WillHeld/roberta-base-mnli\", \"mnli\", \"validation_matched\"),\n",
    "#     (ALTI_OUTPUT_DIR, f\"WillHeld/roberta-base-mnli\", \"mnli\", \"validation_matched\"),\n",
    "#     (IG_OUTPUT_DIR, f\"WillHeld/roberta-base-sst2\", \"sst2\", \"validation\"),\n",
    "#     (IXG_OUTPUT_DIR, f\"WillHeld/roberta-base-sst2\", \"sst2\", \"validation\"),\n",
    "#     (IG_OUTPUT_DIR, f\"WillHeld/roberta-base-mnli\", \"mnli\", \"validation_matched\"),\n",
    "#     (IXG_OUTPUT_DIR, f\"WillHeld/roberta-base-mnli\", \"mnli\", \"validation_matched\"),\n",
    "]\n",
    "\n",
    "GLOBENC_CONFIG_NAMES = [\n",
    "    'Decomposition AbsDot Bias',\n",
    "#     '+Decomposition AbsDot Bias',\n",
    "    'GlobEnc', \n",
    "#     'GlobEnc No Bias FFN', \n",
    "#     'Decomposition No Bias', \n",
    "#     'Decomposition Equal Bias', 'Decomposition Norm Bias', 'Decomposition AbsSim Bias', \n",
    "#     'Decomposition AbsDot Bias No FFN'\n",
    "#     'GlobEnc AbsDot Bias', 'GlobEnc AbsSim Bias', 'GlobEnc Equal Bias', 'GlobEnc Norm Bias', \n",
    "#     'GlobEnc AbsDot Bias FFN', 'GlobEnc Equal Bias FFN', 'GlobEnc AbsSim Bias FFN', 'GlobEnc Norm Bias FFN', \n",
    "#     'Decomposition No Bias No FFN',\n",
    "    \n",
    "    ### IG ###\n",
    "    \"IG_ALL_ZERO_NORM_prediction_based\",\n",
    "#     \"IG_ALL_PAD_SUM_prediction_based\",\n",
    "#     \"IG_ALL_PAD_NORM_prediction_based\",\n",
    "    \"IG_ALL_ZERO_SUM_prediction_based\",\n",
    "#     \"IG_ALL_PAD_SUM_label_based\",\n",
    "#     \"IG_ALL_PAD_NORM_label_based\",\n",
    "#     \"IG_ALL_ZERO_SUM_label_based\",\n",
    "#     \"IG_ALL_ZERO_NORM_label_based\",\n",
    "\n",
    "    ### IXG ###\n",
    "    \"IXG_NORM_prediction_based\",\n",
    "    \"IXG_SUM_prediction_based\",\n",
    "    \n",
    "    ### ALTI ###\n",
    "    \"ALTI\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2668de14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_importance(importance, tokenized_text, discrete=False, prefix=\"\", no_cls_sep=False):\n",
    "    \"\"\"\n",
    "    importance: (sent_len)\n",
    "    \"\"\"\n",
    "    if no_cls_sep:\n",
    "        importance = importance[1:-1]\n",
    "        tokenized_text = tokenized_text[1:-1]\n",
    "    importance = importance / np.abs(importance).max() / 1.5  # Normalize\n",
    "    if discrete:\n",
    "        importance = np.argsort(np.argsort(importance)) / len(importance) / 1.6\n",
    "    \n",
    "    html = \"<pre>\"+prefix\n",
    "#     plt.plot(importance)\n",
    "    for i in range(len(tokenized_text)):\n",
    "        if importance[i] >= 0:\n",
    "            rgba = matplotlib.cm.get_cmap('Greens')(importance[i])   # Wistia\n",
    "        else:\n",
    "            rgba = matplotlib.cm.get_cmap('Reds')(np.abs(importance[i]))   # Wistia\n",
    "        text_color = \"color: rgba(255, 255, 255, 1.0); \" if np.abs(importance[i]) > 0.9 else \"\"\n",
    "        color = f\"background-color: rgba({rgba[0]*255}, {rgba[1]*255}, {rgba[2]*255}, {rgba[3]}); \" + text_color\n",
    "        html += (f\"<span style='\"\n",
    "                 f\"{color}\"\n",
    "                 f\"border-radius: 5px; padding: 3px;\"\n",
    "#                  f\"background-color: rgba(200, {cls_attention[i]*255}, 10, 1.0); \"\n",
    "#                  f\"font-size: {int(cls_attention[i]*18 + 1)}px; \"\n",
    "#                  f\"font-weight: {int(cls_attention[i]*900)};\"\n",
    "                 f\"font-weight: {int(800)};\"\n",
    "                 \"'>\")\n",
    "        html += tokenized_text[i].replace('<', \"[\").replace(\">\", \"]\")\n",
    "        html += \"</span> \"\n",
    "    display(HTML(html))\n",
    "#     print(html)\n",
    "    return html\n",
    "\n",
    "def make_result_filename(task_name, set_of_data, model_checkpoint, cfg_name):\n",
    "    file_name = f\"[{task_name}]_[{set_of_data}]_[{'-'.join(model_checkpoint.split('/')[-2:])}]_[{cfg_name}]\"\n",
    "    return file_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a925ad49",
   "metadata": {},
   "source": [
    "# Preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "74f4fd51",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ec6e34b5b794a41832ae93e192487aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Models_Dataset_Sets:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a499feef8234acaac13b85dbdded45a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Configs:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[sst2]_[validation]_[output_sst2_bert-base-uncased_0001_SEED0042-checkpoint-10525]_[Decomposition AbsDot Bias]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>aggregated:         <span style='background-color: rgba(221.3529411764706, 241.94117647058823, 215.84313725490196, 1.0); border-radius: 5px; padding: 3px;font-weight: 800;'>a</span> <span style='background-color: rgba(55.94117647058824, 161.3372549019608, 85.75294117647059, 1.0); border-radius: 5px; padding: 3px;font-weight: 800;'>good</span> <span style='background-color: rgba(223.23529411764704, 242.69411764705882, 217.85098039215688, 1.0); border-radius: 5px; padding: 3px;font-weight: 800;'>piece</span> <span style='background-color: rgba(236.27058823529413, 247.82745098039217, 232.4823529411765, 1.0); border-radius: 5px; padding: 3px;font-weight: 800;'>of</span> <span style='background-color: rgba(211.00000000000003, 237.79999999999998, 204.79999999999998, 1.0); border-radius: 5px; padding: 3px;font-weight: 800;'>work</span> <span style='background-color: rgba(183.20392156862744, 226.34901960784313, 176.61960784313726, 1.0); border-radius: 5px; padding: 3px;font-weight: 800;'>more</span> <span style='background-color: rgba(212.88235294117646, 238.5529411764706, 206.8078431372549, 1.0); border-radius: 5px; padding: 3px;font-weight: 800;'>often</span> <span style='background-color: rgba(230.05882352941177, 245.41176470588235, 225.23529411764704, 1.0); border-radius: 5px; padding: 3px;font-weight: 800;'>than</span> <span style='background-color: rgba(55.0, 160.33333333333334, 85.0, 1.0); border-radius: 5px; padding: 3px;font-weight: 800;'>not</span> <span style='background-color: rgba(201.58823529411765, 234.03529411764706, 194.7607843137255, 1.0); border-radius: 5px; padding: 3px;font-weight: 800;'>.</span> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>classifierL0|L=1:   <span style='background-color: rgba(253.5529411764706, 215.72941176470587, 199.0470588235294, 1.0); border-radius: 5px; padding: 3px;font-weight: 800;'>a</span> <span style='background-color: rgba(227.0, 47.333333333333336, 39.0, 1.0); border-radius: 5px; padding: 3px;font-weight: 800;'>good</span> <span style='background-color: rgba(253.86666666666665, 221.53333333333333, 206.73333333333332, 1.0); border-radius: 5px; padding: 3px;font-weight: 800;'>piece</span> <span style='background-color: rgba(238.52941176470588, 248.7058823529412, 235.11764705882354, 1.0); border-radius: 5px; padding: 3px;font-weight: 800;'>of</span> <span style='background-color: rgba(252.92549019607844, 204.12156862745098, 183.67450980392158, 1.0); border-radius: 5px; padding: 3px;font-weight: 800;'>work</span> <span style='background-color: rgba(252.0, 176.38823529411764, 148.83529411764707, 1.0); border-radius: 5px; padding: 3px;font-weight: 800;'>more</span> <span style='background-color: rgba(226.05882352941177, 243.82352941176472, 220.8627450980392, 1.0); border-radius: 5px; padding: 3px;font-weight: 800;'>often</span> <span style='background-color: rgba(254.1843137254902, 227.87058823529412, 215.52941176470586, 1.0); border-radius: 5px; padding: 3px;font-weight: 800;'>than</span> <span style='background-color: rgba(75.19999999999999, 176.0, 97.99999999999999, 1.0); border-radius: 5px; padding: 3px;font-weight: 800;'>not</span> <span style='background-color: rgba(204.41176470588235, 235.16470588235293, 197.77254901960785, 1.0); border-radius: 5px; padding: 3px;font-weight: 800;'>.</span> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>classifierL1|L=1:   <span style='background-color: rgba(222.29411764705884, 242.31764705882355, 216.8470588235294, 1.0); border-radius: 5px; padding: 3px;font-weight: 800;'>a</span> <span style='background-color: rgba(58.764705882352935, 164.34901960784313, 88.01176470588234, 1.0); border-radius: 5px; padding: 3px;font-weight: 800;'>good</span> <span style='background-color: rgba(226.05882352941177, 243.82352941176472, 220.8627450980392, 1.0); border-radius: 5px; padding: 3px;font-weight: 800;'>piece</span> <span style='background-color: rgba(254.46666666666667, 233.8, 224.0, 1.0); border-radius: 5px; padding: 3px;font-weight: 800;'>of</span> <span style='background-color: rgba(213.8235294117647, 238.92941176470586, 207.81176470588235, 1.0); border-radius: 5px; padding: 3px;font-weight: 800;'>work</span> <span style='background-color: rgba(182.01176470588234, 225.8470588235294, 175.45882352941177, 1.0); border-radius: 5px; padding: 3px;font-weight: 800;'>more</span> <span style='background-color: rgba(253.67843137254903, 218.05098039215684, 202.12156862745098, 1.0); border-radius: 5px; padding: 3px;font-weight: 800;'>often</span> <span style='background-color: rgba(230.05882352941177, 245.41176470588235, 225.23529411764704, 1.0); border-radius: 5px; padding: 3px;font-weight: 800;'>than</span> <span style='background-color: rgba(227.0, 47.333333333333336, 39.0, 1.0); border-radius: 5px; padding: 3px;font-weight: 800;'>not</span> <span style='background-color: rgba(252.10980392156864, 189.03137254901958, 163.69019607843137, 1.0); border-radius: 5px; padding: 3px;font-weight: 800;'>.</span> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "[sst2]_[validation]_[output_sst2_bert-base-uncased_0001_SEED0042-checkpoint-10525]_[GlobEnc]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>aggregated:         <span style='background-color: rgba(174.85882352941178, 222.83529411764704, 168.49411764705883, 1.0); border-radius: 5px; padding: 3px;font-weight: 800;'>a</span> <span style='background-color: rgba(102.39999999999999, 189.33333333333334, 111.33333333333333, 1.0); border-radius: 5px; padding: 3px;font-weight: 800;'>good</span> <span style='background-color: rgba(108.80000000000005, 192.47058823529417, 114.47058823529414, 1.0); border-radius: 5px; padding: 3px;font-weight: 800;'>piece</span> <span style='background-color: rgba(179.62745098039215, 224.84313725490196, 173.13725490196077, 1.0); border-radius: 5px; padding: 3px;font-weight: 800;'>of</span> <span style='background-color: rgba(146.3529411764706, 210.16470588235293, 142.95686274509805, 1.0); border-radius: 5px; padding: 3px;font-weight: 800;'>work</span> <span style='background-color: rgba(108.80000000000005, 192.47058823529417, 114.47058823529414, 1.0); border-radius: 5px; padding: 3px;font-weight: 800;'>more</span> <span style='background-color: rgba(55.0, 160.33333333333334, 85.0, 1.0); border-radius: 5px; padding: 3px;font-weight: 800;'>often</span> <span style='background-color: rgba(76.8, 176.78431372549016, 98.7843137254902, 1.0); border-radius: 5px; padding: 3px;font-weight: 800;'>than</span> <span style='background-color: rgba(128.0, 201.60000000000002, 127.86666666666666, 1.0); border-radius: 5px; padding: 3px;font-weight: 800;'>not</span> <span style='background-color: rgba(179.62745098039215, 224.84313725490196, 173.13725490196077, 1.0); border-radius: 5px; padding: 3px;font-weight: 800;'>.</span> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb05ad2c5bc142f8b9bbfeb78b58a671",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Configs:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[sst2]_[validation]_[output_sst2_bert-base-uncased_0001_SEED0042-checkpoint-10525]_[IG_ALL_ZERO_NORM_prediction_based]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>aggregated:         <span style='background-color: rgba(147.76470588235293, 210.82352941176472, 144.1176470588235, 1.0); border-radius: 5px; padding: 3px;font-weight: 800;'>a</span> <span style='background-color: rgba(55.0, 160.33333333333334, 85.0, 1.0); border-radius: 5px; padding: 3px;font-weight: 800;'>good</span> <span style='background-color: rgba(73.6, 175.2156862745098, 97.2156862745098, 1.0); border-radius: 5px; padding: 3px;font-weight: 800;'>piece</span> <span style='background-color: rgba(204.41176470588235, 235.16470588235293, 197.77254901960785, 1.0); border-radius: 5px; padding: 3px;font-weight: 800;'>of</span> <span style='background-color: rgba(126.58823529411765, 200.94117647058823, 126.70588235294117, 1.0); border-radius: 5px; padding: 3px;font-weight: 800;'>work</span> <span style='background-color: rgba(176.05098039215687, 223.3372549019608, 169.65490196078431, 1.0); border-radius: 5px; padding: 3px;font-weight: 800;'>more</span> <span style='background-color: rgba(142.1176470588235, 208.18823529411765, 139.47450980392156, 1.0); border-radius: 5px; padding: 3px;font-weight: 800;'>often</span> <span style='background-color: rgba(153.41176470588235, 213.45882352941177, 148.76078431372548, 1.0); border-radius: 5px; padding: 3px;font-weight: 800;'>than</span> <span style='background-color: rgba(182.01176470588234, 225.8470588235294, 175.45882352941177, 1.0); border-radius: 5px; padding: 3px;font-weight: 800;'>not</span> <span style='background-color: rgba(125.17647058823529, 200.2823529411765, 125.54509803921569, 1.0); border-radius: 5px; padding: 3px;font-weight: 800;'>.</span> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "[sst2]_[validation]_[output_sst2_bert-base-uncased_0001_SEED0042-checkpoint-10525]_[IG_ALL_ZERO_SUM_prediction_based]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>aggregated:         <span style='background-color: rgba(254.37254901960785, 231.8235294117647, 221.17647058823528, 1.0); border-radius: 5px; padding: 3px;font-weight: 800;'>a</span> <span style='background-color: rgba(133.64705882352942, 204.23529411764707, 132.50980392156862, 1.0); border-radius: 5px; padding: 3px;font-weight: 800;'>good</span> <span style='background-color: rgba(55.0, 160.33333333333334, 85.0, 1.0); border-radius: 5px; padding: 3px;font-weight: 800;'>piece</span> <span style='background-color: rgba(244.1764705882353, 250.90196078431373, 241.7058823529412, 1.0); border-radius: 5px; padding: 3px;font-weight: 800;'>of</span> <span style='background-color: rgba(183.20392156862744, 226.34901960784313, 176.61960784313726, 1.0); border-radius: 5px; padding: 3px;font-weight: 800;'>work</span> <span style='background-color: rgba(201.58823529411765, 234.03529411764706, 194.7607843137255, 1.0); border-radius: 5px; padding: 3px;font-weight: 800;'>more</span> <span style='background-color: rgba(254.90588235294118, 243.0235294117647, 237.1764705882353, 1.0); border-radius: 5px; padding: 3px;font-weight: 800;'>often</span> <span style='background-color: rgba(225.11764705882354, 243.44705882352943, 219.85882352941175, 1.0); border-radius: 5px; padding: 3px;font-weight: 800;'>than</span> <span style='background-color: rgba(254.05882352941177, 225.23529411764704, 211.76470588235293, 1.0); border-radius: 5px; padding: 3px;font-weight: 800;'>not</span> <span style='background-color: rgba(252.0, 162.2392156862745, 132.6156862745098, 1.0); border-radius: 5px; padding: 3px;font-weight: 800;'>.</span> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad8cbe087751482ca8de728cd4ab7e00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Configs:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[sst2]_[validation]_[output_sst2_bert-base-uncased_0001_SEED0042-checkpoint-10525]_[IXG_NORM_prediction_based]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>aggregated:         <span style='background-color: rgba(203.47058823529412, 234.78823529411764, 196.76862745098038, 1.0); border-radius: 5px; padding: 3px;font-weight: 800;'>a</span> <span style='background-color: rgba(99.2, 187.76470588235293, 109.76470588235294, 1.0); border-radius: 5px; padding: 3px;font-weight: 800;'>good</span> <span style='background-color: rgba(78.39999999999999, 177.5686274509804, 99.56862745098039, 1.0); border-radius: 5px; padding: 3px;font-weight: 800;'>piece</span> <span style='background-color: rgba(215.7058823529412, 239.68235294117648, 209.81960784313728, 1.0); border-radius: 5px; padding: 3px;font-weight: 800;'>of</span> <span style='background-color: rgba(161.74509803921566, 217.31372549019608, 155.7254901960784, 1.0); border-radius: 5px; padding: 3px;font-weight: 800;'>work</span> <span style='background-color: rgba(152.0, 212.79999999999998, 147.59999999999997, 1.0); border-radius: 5px; padding: 3px;font-weight: 800;'>more</span> <span style='background-color: rgba(55.0, 160.33333333333334, 85.0, 1.0); border-radius: 5px; padding: 3px;font-weight: 800;'>often</span> <span style='background-color: rgba(105.6, 190.90196078431373, 112.90196078431373, 1.0); border-radius: 5px; padding: 3px;font-weight: 800;'>than</span> <span style='background-color: rgba(152.0, 212.79999999999998, 147.59999999999997, 1.0); border-radius: 5px; padding: 3px;font-weight: 800;'>not</span> <span style='background-color: rgba(116.70588235294117, 196.3294117647059, 118.58039215686276, 1.0); border-radius: 5px; padding: 3px;font-weight: 800;'>.</span> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "[sst2]_[validation]_[output_sst2_bert-base-uncased_0001_SEED0042-checkpoint-10525]_[IXG_SUM_prediction_based]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>aggregated:         <span style='background-color: rgba(234.01176470588237, 246.94901960784316, 229.84705882352944, 1.0); border-radius: 5px; padding: 3px;font-weight: 800;'>a</span> <span style='background-color: rgba(254.71764705882353, 239.07058823529414, 231.52941176470586, 1.0); border-radius: 5px; padding: 3px;font-weight: 800;'>good</span> <span style='background-color: rgba(254.90588235294118, 243.0235294117647, 237.1764705882353, 1.0); border-radius: 5px; padding: 3px;font-weight: 800;'>piece</span> <span style='background-color: rgba(254.81176470588235, 241.0470588235294, 234.35294117647058, 1.0); border-radius: 5px; padding: 3px;font-weight: 800;'>of</span> <span style='background-color: rgba(253.67843137254903, 218.05098039215684, 202.12156862745098, 1.0); border-radius: 5px; padding: 3px;font-weight: 800;'>work</span> <span style='background-color: rgba(243.28235294117647, 75.7725490196079, 54.70588235294121, 1.0); border-radius: 5px; padding: 3px;font-weight: 800;'>more</span> <span style='background-color: rgba(227.0, 47.333333333333336, 39.0, 1.0); border-radius: 5px; padding: 3px;font-weight: 800;'>often</span> <span style='background-color: rgba(195.12549019607843, 231.3686274509804, 188.22745098039215, 1.0); border-radius: 5px; padding: 3px;font-weight: 800;'>than</span> <span style='background-color: rgba(254.68627450980392, 238.41176470588235, 230.58823529411765, 1.0); border-radius: 5px; padding: 3px;font-weight: 800;'>not</span> <span style='background-color: rgba(177.243137254902, 223.8392156862745, 170.8156862745098, 1.0); border-radius: 5px; padding: 3px;font-weight: 800;'>.</span> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "975fd5bf12544c999246d5ad23a49b7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Configs:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[sst2]_[validation]_[output_sst2_bert-base-uncased_0001_SEED0042-checkpoint-10525]_[ALTI]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>aggregated:         <span style='background-color: rgba(168.89803921568628, 220.32549019607842, 162.69019607843137, 1.0); border-radius: 5px; padding: 3px;font-weight: 800;'>a</span> <span style='background-color: rgba(55.0, 160.33333333333334, 85.0, 1.0); border-radius: 5px; padding: 3px;font-weight: 800;'>good</span> <span style='background-color: rgba(104.0, 190.11764705882354, 112.11764705882354, 1.0); border-radius: 5px; padding: 3px;font-weight: 800;'>piece</span> <span style='background-color: rgba(224.1764705882353, 243.0705882352941, 218.85490196078433, 1.0); border-radius: 5px; padding: 3px;font-weight: 800;'>of</span> <span style='background-color: rgba(153.41176470588235, 213.45882352941177, 148.76078431372548, 1.0); border-radius: 5px; padding: 3px;font-weight: 800;'>work</span> <span style='background-color: rgba(192.74117647058821, 230.36470588235295, 185.90588235294118, 1.0); border-radius: 5px; padding: 3px;font-weight: 800;'>more</span> <span style='background-color: rgba(164.12941176470588, 218.31764705882352, 158.0470588235294, 1.0); border-radius: 5px; padding: 3px;font-weight: 800;'>often</span> <span style='background-color: rgba(174.85882352941178, 222.83529411764704, 168.49411764705883, 1.0); border-radius: 5px; padding: 3px;font-weight: 800;'>than</span> <span style='background-color: rgba(205.3529411764706, 235.54117647058823, 198.7764705882353, 1.0); border-radius: 5px; padding: 3px;font-weight: 800;'>not</span> <span style='background-color: rgba(227.94117647058826, 244.5764705882353, 222.87058823529412, 1.0); border-radius: 5px; padding: 3px;font-weight: 800;'>.</span> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Read Globencs\n",
    "# globencs = dict()\n",
    "\n",
    "def print_preview(idx=0, discrete=False):\n",
    "    NO_CLS_SEP = True\n",
    "    for dir_model_dataset_set in tqdm(DIR_MODEL_DATASET_SET, desc=\"Models_Dataset_Sets\"):\n",
    "        dir, model_checkpoint, task_name, set_of_data = dir_model_dataset_set\n",
    "        for cfg_name in tqdm(GLOBENC_CONFIG_NAMES, desc=\"Configs\"):\n",
    "            file_name = make_result_filename(task_name, set_of_data, model_checkpoint, cfg_name)\n",
    "#             print(f\"{dir}/{file_name}.pkl\")\n",
    "            try:\n",
    "                df = pd.read_pickle(f\"{dir}/{file_name}.pkl\")\n",
    "            except Exception as e:\n",
    "    #             logging.warn(e)\n",
    "                continue\n",
    "\n",
    "            print(file_name)\n",
    "    #         print(\"LOGITS:\", df.head(1))\n",
    "            for col in [\"importance_last_layer_aggregated\", \"importance_last_layer_classifier\"]:\n",
    "                if col in df and df[col][idx] is not None:\n",
    "                    if \"aggregated\" in col:\n",
    "                        sentence_importance = df[col].iloc[idx][0, :]\n",
    "#                         print(sentence_importance)\n",
    "                    if \"classifier\" in col:\n",
    "                        for label in range(df[col].iloc[idx].shape[-1]):\n",
    "                            sentence_importance = df[col].iloc[idx][:, label]\n",
    "#                             print(sentence_importance)\n",
    "                            print_importance(\n",
    "                                sentence_importance,\n",
    "                                df[\"tokens\"].iloc[idx], \n",
    "                                prefix=f\"{col.split('_')[-1]}L{label}|L={df['label'].iloc[idx]}:\".ljust(20),\n",
    "                                no_cls_sep=NO_CLS_SEP,\n",
    "                                discrete=False\n",
    "                            )\n",
    "                        break\n",
    "                        sentence_importance = df[col].iloc[idx][:, df[\"label\"].iloc[idx]]\n",
    "                    if \"pooler\" in col:\n",
    "                        sentence_importance = df[col].iloc[idx]\n",
    "                    print_importance(\n",
    "                        sentence_importance,\n",
    "                        df[\"tokens\"].iloc[idx], \n",
    "                        prefix=f\"{col.split('_')[-1]}:\".ljust(20),\n",
    "                        no_cls_sep=NO_CLS_SEP,\n",
    "                        discrete=discrete\n",
    "                    )\n",
    "            print(\"------------------------------------\")\n",
    "    return df\n",
    "\n",
    "df = print_preview(idx=424)  # 2475\n",
    "# df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0282d889",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"tokens\"].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be288a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ds = datasets.load_dataset(\"glue\", DIR_MODEL_DATASET_SET[0][2], download_config=datasets.DownloadConfig(local_files_only=True))\n",
    "# print(ds)\n",
    "# print(ds[DIR_MODEL_DATASET_SET[0][3]][\"idx\"][:10])\n",
    "# print(\"Check above to be ordered!\")\n",
    "# df[df[\"tokens\"].str.len() < 30].index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b381c65",
   "metadata": {},
   "source": [
    "# MASKING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55486775",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CheckpointToLogit:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_checkpoint: str,\n",
    "        task_name: str,\n",
    "        set_of_data: str,\n",
    "\n",
    "        token_importance,  # (N, seq_len)\n",
    "        masking_ratio: float,\n",
    "        masking_strategy = \"min\",  # max | min\n",
    "        mask_token = \"mask\",  # mask | pad\n",
    "        \n",
    "        save_cls: bool = True,\n",
    "    ) -> pd.DataFrame:\n",
    "        self.model_checkpoint = model_checkpoint\n",
    "        self.task_name = task_name\n",
    "        self.set_of_data = set_of_data\n",
    "        self.save_cls = save_cls\n",
    "        \n",
    "        self.token_importance = token_importance\n",
    "        self.masking_ratio = masking_ratio\n",
    "        self.masking_strategy = masking_strategy\n",
    "        self.mask_token = mask_token\n",
    "    \n",
    "    def retrieve(self) -> pd.DataFrame:\n",
    "        DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        # DATASET\n",
    "        GLUE_TASKS = [\"cola\", \"mnli\", \"mnli-mm\", \"mrpc\", \"qnli\", \"qqp\", \"rte\", \"sst2\", \"stsb\", \"wnli\"]\n",
    "        BATCH_SIZE = 8\n",
    "        MAX_LENGTH = 128\n",
    "        def aggregate_hatexplain(example):\n",
    "            def mode(lst):\n",
    "                return max(set(lst), key=lst.count)\n",
    "            example[\"label\"] = mode(example[\"annotators\"][\"label\"])\n",
    "            example[\"text\"] = \" \".join(example[\"post_tokens\"])\n",
    "            return example\n",
    "        actual_task = \"mnli\" if self.task_name == \"mnli-mm\" else self.task_name\n",
    "        if self.task_name in GLUE_TASKS:\n",
    "            dataset = datasets.load_dataset(\"glue\", actual_task, download_config=datasets.DownloadConfig(local_files_only=True))\n",
    "        elif self.task_name == \"hatexplain\":\n",
    "            dataset = datasets.load_dataset(\"hatexplain\", download_config=datasets.DownloadConfig(local_files_only=True)).map(aggregate_hatexplain)\n",
    "            for part in [\"train\", \"validation\", \"test\"]:\n",
    "                dataset[part] = dataset[part].add_column(\"idx\", [i for i in range(len(dataset[part]))])\n",
    "#         metric = datasets.load_metric('glue', actual_task)\n",
    "        task_to_keys = {\n",
    "            \"cola\": (\"sentence\", None),\n",
    "            \"mnli\": (\"premise\", \"hypothesis\"),\n",
    "            \"mnli-mm\": (\"premise\", \"hypothesis\"),\n",
    "            \"mrpc\": (\"sentence1\", \"sentence2\"),\n",
    "            \"qnli\": (\"question\", \"sentence\"),\n",
    "            \"qqp\": (\"question1\", \"question2\"),\n",
    "            \"rte\": (\"sentence1\", \"sentence2\"),\n",
    "            \"sst2\": (\"sentence\", None),\n",
    "            \"stsb\": (\"sentence1\", \"sentence2\"),\n",
    "            \"wnli\": (\"sentence1\", \"sentence2\"),\n",
    "            \"hatexplain\": (\"text\", None),\n",
    "        }\n",
    "        SENTENCE1_KEY, SENTENCE2_KEY = task_to_keys[self.task_name]\n",
    "        def preprocess_function_wrapped(tokenizer):\n",
    "            def preprocess_function(examples):\n",
    "                # Tokenize the texts\n",
    "                args = (\n",
    "                    (examples[SENTENCE1_KEY],) if SENTENCE2_KEY is None else (examples[SENTENCE1_KEY], examples[SENTENCE2_KEY])\n",
    "                )\n",
    "                result = tokenizer(*args, padding=False, max_length=MAX_LENGTH, truncation=True)\n",
    "                return result\n",
    "            return preprocess_function\n",
    "\n",
    "        def token_id_to_tokens_mapper(tokenizer, sample):\n",
    "            length = len(sample[\"input_ids\"])\n",
    "            return tokenizer.convert_ids_to_tokens(sample[\"input_ids\"])[:length], length\n",
    "        \n",
    "        def mask(example):\n",
    "            length = np.sum(example[\"attention_mask\"])\n",
    "            def top_k_masking():\n",
    "                rank = self.token_importance[example[\"idx\"]][:length].argsort()\n",
    "\n",
    "                # Exclude CLS and SEPs\n",
    "                cls_id = tokenizer.cls_token_id\n",
    "                sep_id = tokenizer.sep_token_id\n",
    "#                 print(\"CLS, SEP:\", cls_id, sep_id)\n",
    "                rank = rank[~np.in1d(rank, np.argwhere(((np.array(example[\"input_ids\"])==cls_id) | (np.array(example[\"input_ids\"])==sep_id)).flatten()))]\n",
    "                \n",
    "                mask_count = int(np.floor(len(rank) * self.masking_ratio))\n",
    "                masks = []\n",
    "                if self.masking_strategy == \"max\":\n",
    "                    masks = rank[-mask_count:]\n",
    "                elif self.masking_strategy == \"min\":\n",
    "                    masks = rank[:mask_count]\n",
    "                else:\n",
    "                    raise Exception(\"Unknow masking_strategy. Options are 'max' or 'min'\")\n",
    "                if mask_count == 0:\n",
    "                    masks = []\n",
    "                return masks\n",
    "            \n",
    "            def top_p_masking():\n",
    "                p = self.token_importance[example[\"idx\"]][:length]\n",
    "                p = p / np.sum(p)\n",
    "                p_argsort = p.argsort()\n",
    "                if self.masking_strategy == \"max\":\n",
    "                    p_argsort = np.flip(p_argsort)\n",
    "                masks = []\n",
    "                cumulative_p = 0.0\n",
    "                for i in range(length):\n",
    "                    if example[\"input_ids\"][p_argsort[i]] >= 103:  # Exclude CLS and SEPs\n",
    "                        cumulative_p += p[p_argsort[i]]\n",
    "                        if cumulative_p <= self.masking_ratio:\n",
    "                            masks.append(p_argsort[i])\n",
    "                return masks\n",
    "            \n",
    "            replacement_token_map = {\n",
    "                \"mask\": tokenizer.mask_token_id,\n",
    "                \"pad\": tokenizer.pad_token_id\n",
    "            }\n",
    "            replacement_token = replacement_token_map[self.mask_token]\n",
    "            masks = top_k_masking()\n",
    "            example[\"input_ids\"] = [replacement_token if j in masks else example[\"input_ids\"][j] for j in range(length)]\n",
    "            return example\n",
    "            \n",
    "        # RUN\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(self.model_checkpoint)\n",
    "        model.eval()\n",
    "        tokenizer = AutoTokenizer.from_pretrained(self.model_checkpoint, use_fast=True, max_length=128)\n",
    "\n",
    "        sel_dataset = dataset[self.set_of_data].map(preprocess_function_wrapped(tokenizer), batched=True, batch_size=1024)\n",
    "        ### MASK ###\n",
    "        sel_dataset = sel_dataset.map(mask)\n",
    "        ### MASK ###\n",
    "        dataset_size = len(sel_dataset)\n",
    "        steps = int(np.ceil(dataset_size / BATCH_SIZE))\n",
    "\n",
    "        globencs = {\n",
    "            \"tokens\": [],\n",
    "            \"logits\": [],\n",
    "            \"labels\": [],\n",
    "        }\n",
    "        lengths = []\n",
    "        \n",
    "        for i in tqdm(range(dataset_size), desc=\"Tokenize Masked\"):\n",
    "            tokens, length = token_id_to_tokens_mapper(tokenizer, sel_dataset[i])\n",
    "            globencs[\"labels\"].append(sel_dataset[i][\"label\"])\n",
    "            globencs[\"tokens\"].append(tokens)\n",
    "            lengths.append(length)\n",
    "\n",
    "        generator = torch.Generator()\n",
    "        generator.manual_seed(int(torch.empty((), dtype=torch.int64).random_().item()))\n",
    "\n",
    "        sampler = LengthGroupedSampler(\n",
    "            BATCH_SIZE,\n",
    "            lengths=lengths,\n",
    "            model_input_name=tokenizer.model_input_names[0],\n",
    "            generator=generator,\n",
    "        )\n",
    "\n",
    "        collator = DataCollatorWithPadding(\n",
    "            tokenizer=tokenizer\n",
    "        )\n",
    "\n",
    "        sel_dataset = sel_dataset.add_column(\"length\", lengths)\n",
    "#         sel_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"token_type_ids\", \"attention_mask\", \"length\", \"idx\"])\n",
    "        cols = [\"input_ids\", \"attention_mask\", \"length\", \"idx\"]\n",
    "        cols = cols + [\"token_type_ids\"] if not \"roberta\" in self.model_checkpoint else cols\n",
    "        sel_dataset.set_format(type=\"torch\", columns=cols)\n",
    "\n",
    "        dataloader = DataLoader(\n",
    "            sel_dataset,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            sampler=sampler,\n",
    "            collate_fn=collator\n",
    "        )\n",
    "\n",
    "        model.to(DEVICE)\n",
    "\n",
    "        it = iter(dataloader)\n",
    "        idxes = []\n",
    "        shuffled_globencs, shuffled_cls, shuffled_globencs_all_layers, shuffled_logits = [], [], [], []\n",
    "        with torch.no_grad():\n",
    "            for i in tqdm(range(steps), desc=\"Forward\"):\n",
    "                batch = next(it)\n",
    "#                 input_batch = {k: batch[k].to(DEVICE) for k in [\"input_ids\", \"token_type_ids\", \"attention_mask\"]}\n",
    "                input_batch = {k: batch[k].to(DEVICE) for k in batch.keys() - ['idx', 'length']}\n",
    "                logits, hidden_states = model(\n",
    "                    **input_batch, \n",
    "                    output_attentions=False, \n",
    "                    return_dict=False, \n",
    "                    output_hidden_states=True, \n",
    "                )\n",
    "                batch_lengths = batch[\"length\"].numpy()\n",
    "                idxes.extend(batch['idx'].tolist())\n",
    "                shuffled_logits.extend(logits.cpu().numpy())\n",
    "                if self.save_cls:\n",
    "                    cls_repr = hidden_states[-1][:, 0, :].cpu().numpy()  # [13, batch, len, 768]\n",
    "                    shuffled_cls.extend(cls_repr)\n",
    "\n",
    "        inverse_idxes = np.argsort(idxes)\n",
    "        globencs[\"logits\"] = [shuffled_logits[inverse_idxes[i]] for i in range(dataset_size)]\n",
    "        globencs[\"label\"] = sel_dataset[\"label\"]\n",
    "        if self.save_cls:\n",
    "            globencs[\"cls\"] = [shuffled_cls[inverse_idxes[i]] for i in range(dataset_size)]\n",
    "        return pd.DataFrame(globencs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c564346",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Results:\n",
    "    def __init__(self, strategy=\"max\"):\n",
    "        # data\n",
    "        self.data = dict()\n",
    "        # indices\n",
    "        self.indices = {\n",
    "            \"masking_ratios\": set(), \n",
    "            \"model_checkpoint\": set(), \n",
    "            \"task_name\": set(), \n",
    "            \"set_of_data\": set(), \n",
    "            \"globenc_config_name\": set(),\n",
    "            \"output_type\": set(),\n",
    "        }\n",
    "        # Saving path\n",
    "        self.save_path = f\"results-{strategy}.pkl\"\n",
    "        self.load()\n",
    "    \n",
    "    def _make_index_name(self, index_to_value: dict):\n",
    "        return \"_\".join([f\"[{k}:{v}]\" for k, v in index_to_value.items()])\n",
    "    \n",
    "    def add(self, values, index_to_value: dict):\n",
    "        self.data[self._make_index_name(index_to_value)] = values\n",
    "        for key in self.indices.keys():\n",
    "            self.indices[key].add(index_to_value[key])\n",
    "#         self.save()\n",
    "        \n",
    "    def get(self, index_to_value: dict):\n",
    "        result = self.data[self._make_index_name(index_to_value)]\n",
    "        return result\n",
    "    \n",
    "    def summary(self):\n",
    "        return self.indices\n",
    "    \n",
    "    def save(self):\n",
    "        with open(self.save_path, 'wb') as f:\n",
    "            print(f\"Saving {self.save_path} ...\")\n",
    "            pickle.dump(self.__dict__, f, pickle.HIGHEST_PROTOCOL)\n",
    "            print(f\"Saved {self.save_path}\")\n",
    "    \n",
    "    def load(self):\n",
    "        try:\n",
    "            with open(self.save_path, 'rb') as f:\n",
    "                print(f\"Loading {self.save_path} ...\")\n",
    "                self.__dict__ = pickle.load(f)\n",
    "                print(f\"Loaded {self.save_path}\")\n",
    "        except:\n",
    "            print(f\"Could not load past results from {self.save_path}!\")\n",
    "\n",
    "\n",
    "MASKING_STRATEGY = \"max\"  # max | min\n",
    "results = Results(MASKING_STRATEGY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feadbd37",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "MASKING_RATIOS = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]  # Must include 0.0 for AOPC  [0.0, 0.2, 0.5, 0.7, 0.8, 0.9] [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "for masking_ratio in tqdm(MASKING_RATIOS, desc=\"Ratios\"):\n",
    "    for dir_model_dataset_set in tqdm(DIR_MODEL_DATASET_SET, desc=\"Models_Dataset_Sets\"):\n",
    "        dir, model_checkpoint, task_name, set_of_data = dir_model_dataset_set\n",
    "        for gcfg in tqdm(GLOBENC_CONFIG_NAMES, desc=\"Configs\"):\n",
    "            file_name = f\"[{task_name}]_[{set_of_data}]_[{'-'.join(model_checkpoint.split('/')[-2:])}]_[{gcfg}]\"\n",
    "            print(\"Reading\", file_name)\n",
    "            try:\n",
    "                df = pd.read_pickle(f\"{dir}/{file_name}.pkl\")\n",
    "            except Exception as e:\n",
    "                logging.warn(e)\n",
    "                continue\n",
    "            for col in tqdm([\"importance_last_layer_aggregated\", \"importance_last_layer_classifier\"]):\n",
    "                if col not in df or df[col][0] is None:  # For globenc/IG which has no classificier\n",
    "                    continue\n",
    "                if \"aggregated\" in col:\n",
    "                    token_importance = [v[0, :] for v in df[col].values]\n",
    "                if \"classifier\" in col:\n",
    "#                     token_importance = [v[:, df[\"label\"].iloc[i]] for i, v in enumerate(df[col].values)]\n",
    "                    token_importance = [row[col][:, row[\"logits\"].argmax()] for i, row in df.iterrows()]\n",
    "                idx = 0\n",
    "                print_importance(\n",
    "                    token_importance[idx], df[\"tokens\"].iloc[idx], prefix=f\"{gcfg}_{col.split('_')[-1]}:\".ljust(13), \n",
    "                    no_cls_sep=False,\n",
    "                    discrete=False\n",
    "                )\n",
    "                ctl = CheckpointToLogit(\n",
    "                    model_checkpoint=model_checkpoint,\n",
    "                    task_name=task_name,\n",
    "                    set_of_data=set_of_data,\n",
    "                    save_cls=False,\n",
    "                    token_importance=token_importance,\n",
    "                    masking_ratio=masking_ratio,\n",
    "                    masking_strategy = MASKING_STRATEGY,  # max | min\n",
    "                    mask_token = \"mask\",  # mask | pad\n",
    "                )\n",
    "                mask_df = ctl.retrieve()\n",
    "                idx_values = {\n",
    "                    \"masking_ratios\": masking_ratio, \n",
    "                    \"model_checkpoint\": model_checkpoint, \n",
    "                    \"task_name\": task_name, \n",
    "                    \"set_of_data\": set_of_data, \n",
    "                    \"globenc_config_name\": gcfg,\n",
    "                    \"output_type\": col.split(\"_\")[-1]\n",
    "                }\n",
    "                results.add(mask_df, idx_values)\n",
    "                print(idx_values)\n",
    "                display(mask_df.head(1))\n",
    "results.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55be4c3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def compute_aopc(results: Results, index_to_value: dict):\n",
    "    result = results.get(index_to_value)\n",
    "    logits = scipy.special.softmax(np.array(result[\"logits\"].tolist()), axis=-1)\n",
    "    \n",
    "    base_index_to_value = index_to_value.copy()\n",
    "    base_index_to_value[\"masking_ratios\"] = 0.0\n",
    "    base_result = results.get(base_index_to_value)\n",
    "    base_logits = scipy.special.softmax(np.array(base_result[\"logits\"].tolist()), axis=-1)\n",
    "    \n",
    "    p_hat_idx = np.expand_dims(base_logits.argmax(axis=-1), -1)  # (N, 1)\n",
    "    masked_p = np.take_along_axis(logits, p_hat_idx, axis=-1)  # Select p of the predicted label\n",
    "    base_p = np.take_along_axis(base_logits, p_hat_idx, axis=-1)  # Select p of the predicted label\n",
    "    aopc = np.mean(base_p - masked_p)\n",
    "#     aopc = np.mean(np.abs(base_p - masked_p))\n",
    "    return aopc\n",
    "\n",
    "def compute_accuracy(results: Results, index_to_value: dict):\n",
    "    result = results.get(index_to_value)\n",
    "    preds = np.argmax(np.array(result[\"logits\"].tolist()), axis=-1)\n",
    "    labels = result[\"label\"].values\n",
    "    return accuracy_score(y_true=labels, y_pred=preds)\n",
    "\n",
    "def compute_lodds(results: Results, index_to_value: dict):\n",
    "    result = results.get(index_to_value)\n",
    "    logits = scipy.special.softmax(np.array(result[\"logits\"].tolist()), axis=-1)\n",
    "    \n",
    "    base_index_to_value = index_to_value.copy()\n",
    "    base_index_to_value[\"masking_ratios\"] = 0.0\n",
    "    base_result = results.get(base_index_to_value)\n",
    "    base_logits = scipy.special.softmax(np.array(base_result[\"logits\"].tolist()), axis=-1)\n",
    "    \n",
    "    p_hat_idx = np.expand_dims(base_logits.argmax(axis=-1), -1)  # (N, 1)\n",
    "    masked_p = np.take_along_axis(logits, p_hat_idx, axis=-1)  # Select p of predicted label\n",
    "    base_p = np.take_along_axis(base_logits, p_hat_idx, axis=-1)  # Select p of predicted label\n",
    "    lodds = np.mean(np.log(masked_p / base_p))\n",
    "    return lodds\n",
    "\n",
    "def compute_mae(results: Results, index_to_value: dict):\n",
    "    result = results.get(index_to_value)\n",
    "    logits = scipy.special.softmax(np.array(result[\"logits\"].tolist()), axis=-1)\n",
    "    base_index_to_value = index_to_value.copy()\n",
    "    base_index_to_value[\"masking_ratios\"] = 0.0\n",
    "    base_result = results.get(base_index_to_value)\n",
    "    base_logits = scipy.special.softmax(np.array(base_result[\"logits\"].tolist()), axis=-1)\n",
    "    \n",
    "    p_hat_idx = np.expand_dims(base_logits.argmax(axis=-1), -1)  # (N, 1)\n",
    "    masked_p = np.take_along_axis(logits, p_hat_idx, axis=-1)  # Select p of predicted label\n",
    "    base_p = np.take_along_axis(base_logits, p_hat_idx, axis=-1)  # Select p of predicted label\n",
    "    mae = np.mean(np.abs(masked_p - base_p))\n",
    "    return mae\n",
    "\n",
    "metric_function = {\n",
    "    \"Accuracy\": compute_accuracy,\n",
    "    \"AOPC\": compute_aopc,\n",
    "    \"LOdds\": compute_lodds,\n",
    "    \"AOPC+\": compute_mae,\n",
    "}\n",
    "METRIC = \"Accuracy\"\n",
    "filter_dataset = \"sst2\"  # None sst2 mnli qnli hatexplain\n",
    "filter_exclude_configs = [] # [\"Decomposition Equal Bias\", \"Decomposition Norm Bias\", \"Decomposition ReLU\"]\n",
    "\n",
    "plt.figure(figsize=(11, 8))\n",
    "LEGEND_MAP = {\n",
    "    \"aggregated Decomposition\": \"Decomposition\",\n",
    "#     \"aggregated GlobEnc\": \"GlobEnc\",\n",
    "#     \"aggregated IG_ALL_PAD_SUM_prediction_based\": \"Integrated Gradient\",\n",
    "    \"aggregated IXG_NORM_prediction_based\": \"GradientInput\",\n",
    "    \"classifier Decomposition\": \"Decomposition + Classification Head\",\n",
    "}\n",
    "markers = ['-D', '-o', '-^', '-*', '-s', '-+', '-x', '-.', '-X']\n",
    "cmap = plt.get_cmap(\"tab10\")\n",
    "c = 0\n",
    "for output_type in sorted(results.indices['output_type']):\n",
    "    for globenc_config_name in tqdm(sorted(results.indices['globenc_config_name'])):\n",
    "        for model_checkpoint in results.indices['model_checkpoint']:\n",
    "            for task_name in results.indices['task_name']:\n",
    "                for set_of_data in results.indices['set_of_data']:\n",
    "                    if filter_dataset and filter_dataset not in f\"{task_name}_{globenc_config_name}_{output_type}\":\n",
    "                        continue\n",
    "#                     if any([f in globenc_config_name for f in filter_exclude_configs]):\n",
    "#                         continue\n",
    "#                     if output_type == \"aggregated\":\n",
    "#                         continue\n",
    "                    plot_y, plot_x = [], []\n",
    "                    try:\n",
    "                        for masking_ratio in sorted(results.indices['masking_ratios']):\n",
    "                            if masking_ratio == 1.0:\n",
    "                                continue\n",
    "                            index_to_value = {\n",
    "                                \"masking_ratios\": masking_ratio, \n",
    "                                \"model_checkpoint\": model_checkpoint, \n",
    "                                \"task_name\": task_name, \n",
    "                                \"set_of_data\": set_of_data, \n",
    "                                \"globenc_config_name\": globenc_config_name,\n",
    "                                \"output_type\": output_type,\n",
    "                            }\n",
    "                            metric_result = metric_function[METRIC](results, index_to_value)\n",
    "                            plot_y.append(metric_result)\n",
    "                            plot_x.append(masking_ratio)\n",
    "                        label = f\"{task_name} {output_type} {globenc_config_name}\"\n",
    "                        label = LEGEND_MAP.get(label, label)\n",
    "#                         print(label)\n",
    "                        if (\"Decomp\" in label or \"ALTI\" in label or \"GlobEnc\" in label) and \"roberta\" in model_checkpoint:\n",
    "#                             print(label)\n",
    "                            plt.plot(plot_x, plot_y, markers[c], color=cmap(c), label=label)\n",
    "                            c += 1\n",
    "                        print(f\"{output_type} {globenc_config_name}\".ljust(40), f\"--> mean={np.mean(plot_y[1:])}, {len(plot_y[1:])}\")\n",
    "                    except Exception as e:\n",
    "#                         logginginging.warning(e)\n",
    "                        pass\n",
    "plt.legend(facecolor='white', framealpha=0.8)\n",
    "plt.title(f\"{filter_dataset} Mask {MASKING_STRATEGY} top_k\")\n",
    "plt.ylabel(f\"{METRIC}\")\n",
    "plt.xlabel(\"k\")\n",
    "plt.show()\n",
    "print(model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f615b3a",
   "metadata": {},
   "source": [
    "# Extreme Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb2c7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "output_type = 'aggregated'  # 'classifier'\n",
    "\n",
    "# i1 = {\n",
    "#     'masking_ratios': 0.9, \n",
    "#     'model_checkpoint': '/home/modaresi/projects/globenc_analysis/outputs/models/output_qnli_bert-base-uncased_0001_SEED0042/checkpoint-16370', \n",
    "#     'task_name': 'qnli', \n",
    "#     'set_of_data': 'validation', \n",
    "#     'globenc_config_name': 'Decomposition No Bias', \n",
    "#     'output_type': output_type\n",
    "# }\n",
    "# i2 = {\n",
    "#     'masking_ratios': 0.9, \n",
    "#     'model_checkpoint': '/home/modaresi/projects/globenc_analysis/outputs/models/output_qnli_bert-base-uncased_0001_SEED0042/checkpoint-16370', \n",
    "#     'task_name': 'qnli', \n",
    "#     'set_of_data': 'validation', \n",
    "#     'globenc_config_name': 'Decomposition AbsDot Bias', \n",
    "#     'output_type': output_type\n",
    "# }\n",
    "# df1 = pd.read_pickle(\"/home/modaresi/projects/globenc_analysis/outputs/globencs_v3/[qnli]_[validation]_[output_qnli_bert-base-uncased_0001_SEED0042-checkpoint-16370]_[Decomposition No Bias].pkl\")\n",
    "# df2 = pd.read_pickle(\"/home/modaresi/projects/globenc_analysis/outputs/globencs_v3/[qnli]_[validation]_[output_qnli_bert-base-uncased_0001_SEED0042-checkpoint-16370]_[Decomposition AbsDot Bias].pkl\")\n",
    "\n",
    "i1 = {\n",
    "    'masking_ratios': 0.7, \n",
    "    'model_checkpoint': '/home/modaresi/projects/globenc_analysis/outputs/models/output_mnli_bert-base-uncased_0001_SEED0042/checkpoint-61360', \n",
    "    'task_name': 'mnli', \n",
    "    'set_of_data': 'validation_matched', \n",
    "    'globenc_config_name': 'Decomposition No Bias', \n",
    "    'output_type': output_type\n",
    "}\n",
    "i2 = {\n",
    "    'masking_ratios': 0.7, \n",
    "    'model_checkpoint': '/home/modaresi/projects/globenc_analysis/outputs/models/output_mnli_bert-base-uncased_0001_SEED0042/checkpoint-61360', \n",
    "    'task_name': 'mnli', \n",
    "    'set_of_data': 'validation_matched', \n",
    "    'globenc_config_name': 'Decomposition AbsDot Bias', \n",
    "    'output_type': output_type\n",
    "}\n",
    "df1 = pd.read_pickle(\"/home/modaresi/projects/globenc_analysis/outputs/globencs_v3/[mnli]_[validation_matched]_[output_mnli_bert-base-uncased_0001_SEED0042-checkpoint-61360]_[GlobEnc].pkl\")\n",
    "df2 = pd.read_pickle(\"/home/modaresi/projects/globenc_analysis/outputs/globencs_v3/[mnli]_[validation_matched]_[output_mnli_bert-base-uncased_0001_SEED0042-checkpoint-61360]_[Decomposition AbsDot Bias].pkl\")\n",
    "\n",
    "\n",
    "r1 = results.get(i1)\n",
    "l1 = scipy.special.softmax(np.array(r1[\"logits\"].tolist()), axis=-1)\n",
    "# l1 = np.array(r1[\"logits\"].tolist())\n",
    "\n",
    "r2 = results.get(i2)\n",
    "l2 = scipy.special.softmax(np.array(r2[\"logits\"].tolist()), axis=-1)\n",
    "# l2 = np.array(r2[\"logits\"].tolist())\n",
    "              \n",
    "# np.argsort(np.abs(l1[:, 1] - l2[:, 0]))[-20:]\n",
    "distances = []\n",
    "for i in tqdm(range(len(df1))):\n",
    "#     p = stats.spearmanr(df1.iloc[i][\"importance_last_layer_aggregated\"][:, 1], df2.iloc[i][\"importance_last_layer_aggregated\"][:, 1])[0]\n",
    "#     p = stats.spearmanr(df1.iloc[i][\"importance_last_layer_aggregated\"][0, :], df2.iloc[i][\"importance_last_layer_aggregated\"][0, :])[0]\n",
    "    p = np.max(df2.iloc[i][\"importance_last_layer_aggregated\"][2, :]) - np.min(df2.iloc[i][\"importance_last_layer_aggregated\"][2, :])\n",
    "    distances.append(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da85674c",
   "metadata": {},
   "outputs": [],
   "source": [
    "l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d683be7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for i in np.argsort(np.abs(l1[:, 2] - l2[:, 2]))[-50:]:\n",
    "for i in np.argsort(distances)[-20:]:\n",
    "    print(distances[i], l1[i], l2[i])\n",
    "    if len(r1.iloc[i][\"tokens\"]) < 28 and np.argmax(r2.iloc[i][\"logits\"]) == r2.iloc[i][\"label\"]:\n",
    "        print(\"###### \", i)\n",
    "        print_preview(idx=i, discrete=False)\n",
    "        print(\"No Bias\")\n",
    "        print(r1.iloc[i][\"tokens\"])\n",
    "        print(r1.iloc[i][\"logits\"])\n",
    "        print(\"AbsDot\")\n",
    "        print(r2.iloc[i][\"tokens\"])\n",
    "        print(r2.iloc[i][\"logits\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ab035f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68e9256",
   "metadata": {},
   "outputs": [],
   "source": [
    "### No Bias\n",
    "print(r1.iloc[3023][\"tokens\"])\n",
    "print(r1.iloc[3023][\"logits\"])\n",
    "### AbsDot\n",
    "print(r2.iloc[3023][\"tokens\"])\n",
    "print(r2.iloc[3023][\"logits\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aafc432",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cf54d678",
   "metadata": {},
   "source": [
    "# Final Figures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f30426c",
   "metadata": {},
   "source": [
    "## 1) Main Figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1409c5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_main_fig(METRIC, filter_dataset, LEGEND_CONFIG_MAP, model=\"bert-base\", save_fig=True):\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    configs = [\n",
    "        \"Decomposition AbsDot Bias\", \"ALTI\", \n",
    "        \"IG_ALL_ZERO_NORM_prediction_based\", \"IXG_NORM_prediction_based\", \n",
    "        \"GlobEnc\", \"GlobEnc No Bias FFN\",\n",
    "        \"Decomposition No Bias\", \"Decomposition AbsDot Bias No FFN\", \n",
    "        \"Decomposition AbsSim Bias\", \"Decomposition Norm Bias\", \"Decomposition Equal Bias\",\n",
    "        \"+Decomposition AbsDot Bias\", \"Decomposition No Bias No FFN\",\n",
    "    ]\n",
    "    markers = ['-o', '-D', '-s', '-^', '-*', '-+', '-x', '-.', '-X']\n",
    "    cmap = plt.get_cmap(\"tab10\")\n",
    "    c = 0\n",
    "    aggregated_results = {}\n",
    "    for globenc_config_name in tqdm(configs):\n",
    "        for output_type in results.indices['output_type']:\n",
    "            for model_checkpoint in results.indices['model_checkpoint']:\n",
    "                for task_name in results.indices['task_name']:\n",
    "                    for set_of_data in results.indices['set_of_data']:\n",
    "                        if filter_dataset and filter_dataset not in f\"{task_name}_{globenc_config_name}_{output_type}\" or model not in model_checkpoint:\n",
    "                            continue\n",
    "                        plot_y, plot_x = [], []\n",
    "                        try:\n",
    "                            for masking_ratio in sorted(results.indices['masking_ratios']):\n",
    "                                if masking_ratio == 1.0:\n",
    "                                    continue\n",
    "                                index_to_value = {\n",
    "                                    \"masking_ratios\": masking_ratio, \n",
    "                                    \"model_checkpoint\": model_checkpoint, \n",
    "                                    \"task_name\": task_name, \n",
    "                                    \"set_of_data\": set_of_data, \n",
    "                                    \"globenc_config_name\": globenc_config_name,\n",
    "                                    \"output_type\": output_type,\n",
    "                                }\n",
    "                                metric_result = metric_function[METRIC](results, index_to_value)\n",
    "                                plot_y.append(metric_result)\n",
    "                                plot_x.append(masking_ratio)\n",
    "                            label = f\"{output_type} {globenc_config_name}\"\n",
    "                            \n",
    "#                             print(label)\n",
    "                            if label in LEGEND_CONFIG_MAP.keys():\n",
    "                                label = LEGEND_CONFIG_MAP.get(label, label)\n",
    "                                aggregated_results[label] = np.mean(plot_y[1:])\n",
    "#                                 print(f\"{task_name} {output_type} {globenc_config_name}\".ljust(50), f\"--> mean={np.mean(plot_y[1:]):.3f}|{np.mean(plot_y[1:])*100:.2f}, {len(plot_y[1:])}\")\n",
    "                                print(f\"{METRIC}/{task_name}/{MASKING_STRATEGY}/{label}\".ljust(30), f\"--> mean={np.mean(plot_y[1:]):.3f}|{np.mean(plot_y[1:])*100:.2f}, {len(plot_y[1:])}\")\n",
    "#                                 print([f\"{(y * 100):.2f}%\" for y in plot_y])\n",
    "                                lw = 3 if \"DecompX\" in label else 1\n",
    "                                ms = 8 if \"DecompX\" in label else 6\n",
    "                                if METRIC == \"Accuracy\":\n",
    "                                    plot_y = [y * 100 for y in plot_y]\n",
    "                                plt.plot([f\"{int(x * 100)}%\" for x in plot_x], plot_y, markers[c], color=cmap(c), label=label, lw=lw, markersize=ms)\n",
    "                                c += 1\n",
    "                            else:\n",
    "                                pass\n",
    "                        except Exception as e:\n",
    "    #                         logginginging.warning(e)\n",
    "                            pass\n",
    "    plt.legend(facecolor='white', framealpha=0.8)\n",
    "    plt.title(f\"Mask K% of the {'Least' if MASKING_STRATEGY == 'min' else 'Most'} Important Tokens - {filter_dataset.upper()}\", fontsize=13)\n",
    "    plt.ylabel(f\"{METRIC}\")\n",
    "    plt.xlabel(\"K%\")\n",
    "    if save_fig:\n",
    "        plt.savefig(f\"figs/main_{model}_{filter_dataset}_{MASKING_STRATEGY}_{METRIC}.pdf\", bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(model_checkpoint)\n",
    "    return aggregated_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b411edc2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "LEGEND_CONFIG_MAP = {\n",
    "    \"classifier Decomposition AbsDot Bias\": \"DecompX\",\n",
    "#     \"classifier +Decomposition AbsDot Bias\": \"+DecompX\",\n",
    "    \"aggregated IG_ALL_ZERO_NORM_prediction_based\": \"Integrated Gradient\",\n",
    "    \"aggregated IXG_NORM_prediction_based\": \"GradientInput\",\n",
    "    \"aggregated GlobEnc\": \"GlobEnc\",\n",
    "#     \"aggregated GlobEnc No Bias FFN\": \"GlobEnc FFN\",\n",
    "    \"aggregated ALTI\": \"ALTI\",\n",
    "}\n",
    "for filter_dataset in ['sst2', 'mnli', 'qnli', 'hatexplain']:  # 'sst2', 'mnli', 'qnli', 'hatexplain'\n",
    "    for METRIC in [\"AOPC\", \"Accuracy\"]:  # \"Accuracy\", \"AOPC+\", \"LOdds\", \"AOPC\"\n",
    "        a = plot_main_fig(METRIC, filter_dataset, LEGEND_CONFIG_MAP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94949ef",
   "metadata": {},
   "source": [
    "## 2) Ablation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebc830a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "LEGEND_CONFIG_MAP = {\n",
    "#     \"aggregated GlobEnc\": \"GlobEnc\",\n",
    "    \"classifier Decomposition AbsDot Bias\": \"DecompX\",\n",
    "    \"aggregated Decomposition AbsDot Bias\": \"DecompX W/O Clf. Head\",\n",
    "    \"classifier Decomposition No Bias\": \"DecompX W/O Bias\",\n",
    "    \"classifier Decomposition AbsDot Bias No FFN\": \"DecompX W/O FFN\",\n",
    "#     \"classifier Decomposition No Bias No FFN\": \"DecompX - FFN - Bias\",\n",
    "#     \"aggregated Decomposition No Bias No FFN\": \"DecompX - FFN - Bias - Classification Head\",\n",
    "}\n",
    "df = pd.DataFrame()\n",
    "for filter_dataset in ['mnli', 'hatexplain', 'sst2', 'qnli', ]:  # 'sst2', 'mnli', 'qnli', 'hatexplain'\n",
    "    for METRIC in [\"AOPC\"]:  # \"Accuracy\", \"AOPC+\", \"LOdds\", \"AOPC\"\n",
    "        aggregated_results = plot_main_fig(METRIC, filter_dataset, LEGEND_CONFIG_MAP, save_fig=False)\n",
    "        new_df = pd.DataFrame(aggregated_results.items(), columns=['Method', 'AOPC'])\n",
    "        new_df[\"Dataset\"] = filter_dataset.upper()\n",
    "        print(new_df)\n",
    "        df = df.append(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7196e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = [\"...\", \"xxx\", \"++\", '///', \"|\" , \"-\" , \"+\" , \"//\",  \"O\",  \"*\" ][:4]\n",
    "l = ['DecompX W/O Clf. Head', 'DecompX W/O FFN', 'DecompX W/O Bias', 'DecompX', ]\n",
    "dfp = df.pivot(\"Dataset\", \"Method\", \"AOPC\")[l].loc[[\"QNLI\", \"SST2\", \"HATEXPLAIN\", \"MNLI\"]]\n",
    "ax = dfp.plot(\n",
    "    kind='barh', figsize=(6, 4), width=0.85, \n",
    "#     color=plt.get_cmap(\"tab20b\").colors[4:8][::-1], \n",
    "    color=[plt.get_cmap(\"summer\")(c) for c in [0, 0.3, 0.5, 0.7]][::1], \n",
    "    edgecolor=(1,1,1, 0.5)\n",
    ")\n",
    "# plt.legend(loc='best')\n",
    "# for c, container in enumerate(ax.containers):\n",
    "#     bl = ax.bar_label(\n",
    "#         container, \n",
    "#         labels=[l[c]] * len(l),\n",
    "#         label_type=\"edge\", color=\"black\", padding=0, weight='normal'\n",
    "#     )\n",
    "#     for b in bl:\n",
    "#         b.xy = (0.005, b.xy[1])\n",
    "# ax.get_legend().remove()\n",
    "\n",
    "### HATCH ###\n",
    "# bars = ax.patches\n",
    "# hatches = [h for h in patterns for _ in range(len(dfp))]\n",
    "# print(hatches)\n",
    "# for bar, hatch in zip(bars, hatches):\n",
    "#     bar.set_hatch(hatch)\n",
    "### HATCH ###\n",
    "\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "ax.legend(handles[::-1], labels[::-1], title=None, loc='upper left', facecolor='white', framealpha=0.8)\n",
    "\n",
    "plt.xlim(0, 0.73)\n",
    "plt.xlabel(\"AOPC\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"figs/ablation_leaveOneOut_{MASKING_STRATEGY}_{METRIC}.pdf\", bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a284b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "LEGEND_CONFIG_MAP = {\n",
    "    \"aggregated GlobEnc\": \"GlobEnc\",\n",
    "#     \"classifier Decomposition AbsDot Bias\": \"DecompX\",\n",
    "    \"aggregated Decomposition AbsDot Bias\": \"DecompX W/O Clf. Head\",\n",
    "#     \"classifier Decomposition No Bias\": \"DecompX W/O Bias\",\n",
    "#     \"classifier Decomposition AbsDot Bias No FFN\": \"DecompX W/O FFN\",\n",
    "#     \"classifier Decomposition No Bias No FFN\": \"DecompX W/O FFN/Bias\",\n",
    "    \"aggregated Decomposition No Bias No FFN\": \"DecompX W/O FFN/Bias/Clf. Head\",\n",
    "    \"aggregated GlobEnc No Bias FFN\": \"GlobEnc FFN\",\n",
    "}\n",
    "df = pd.DataFrame()\n",
    "for filter_dataset in ['sst2', 'mnli', 'qnli', 'hatexplain']:  # 'sst2', 'mnli', 'qnli', 'hatexplain'\n",
    "    for METRIC in [\"AOPC\"]:  # \"Accuracy\", \"AOPC+\", \"LOdds\", \"AOPC\"\n",
    "        aggregated_results = plot_main_fig(METRIC, filter_dataset, LEGEND_CONFIG_MAP, save_fig=False)\n",
    "        new_df = pd.DataFrame(aggregated_results.items(), columns=['Method', 'AOPC'])\n",
    "        new_df[\"Dataset\"] = filter_dataset.upper()\n",
    "        print(new_df)\n",
    "        df = df.append(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35fd9fa",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "l = ['GlobEnc', \"GlobEnc FFN\", 'DecompX W/O FFN/Bias/Clf. Head', \"DecompX W/O Clf. Head\"]\n",
    "dfp = df.pivot(\"Dataset\", \"Method\", \"AOPC\")[l].loc[[\"SST2\", \"QNLI\", \"HATEXPLAIN\", \"MNLI\"]]\n",
    "ax = dfp.plot(\n",
    "    kind='barh', figsize=(6, 4), width=0.85, \n",
    "#     color=plt.get_cmap(\"tab20b\").colors[4:8][::-1], \n",
    "    color=[plt.get_cmap(\"plasma\")(c) for c in [0.8, 0.9]] + [plt.get_cmap(\"summer\")(c) for c in [0.6, 0.0]], \n",
    "    edgecolor=(1,1,1, 0.5)\n",
    ")\n",
    "# plt.legend(loc='best')\n",
    "# for c, container in enumerate(ax.containers):\n",
    "#     bl = ax.bar_label(\n",
    "#         container, \n",
    "#         labels=[l[c]] * len(l),\n",
    "#         label_type=\"edge\", color=\"white\", padding=0, weight='bold'\n",
    "#     )\n",
    "#     for b in bl:\n",
    "#         b.xy = (0.005, b.xy[1])\n",
    "# ax.get_legend().remove()\n",
    "\n",
    "### HATCH ###\n",
    "# bars = ax.patches\n",
    "# hatches = [h for h in patterns for _ in range(len(dfp))]\n",
    "# print(hatches)\n",
    "# for bar, hatch in zip(bars, hatches):\n",
    "#     bar.set_hatch(hatch)\n",
    "### HATCH ###\n",
    "\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "ax.legend(handles[::-1], labels[::-1], title=None, loc='upper left', facecolor='white', framealpha=0.8)\n",
    "plt.xlabel(\"AOPC\")\n",
    "plt.xlim(0, 0.73)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"figs/ablation_decomp_{MASKING_STRATEGY}_{METRIC}.pdf\", bbox_inches='tight')\n",
    "plt.show()\n",
    "dfp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18b077d",
   "metadata": {},
   "outputs": [],
   "source": [
    "(red, *plt.get_cmap(\"tab10\").colors[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea13555c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.get_cmap(\"tab10\").colors[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0cf76e",
   "metadata": {},
   "source": [
    "## 3) Biases Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb7d1ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "LEGEND_CONFIG_MAP = {\n",
    "    \"classifier Decomposition AbsDot Bias\": \"DecompX AbsDot Bias\",\n",
    "    \"classifier Decomposition AbsSim Bias\": \"DecompX AbsSim Bias\",\n",
    "    \"classifier Decomposition No Bias\": \"DecompX No Bias\",\n",
    "    \"classifier Decomposition Equal Bias\": \"DecompX Equal Bias\",\n",
    "    \"classifier Decomposition Norm Bias\": \"DecompX Norm Bias\",\n",
    "}\n",
    "for filter_dataset in ['sst2', 'mnli', 'qnli', 'hatexplain']:  # 'sst2', 'mnli', 'qnli', 'hatexplain'\n",
    "    for METRIC in [\"Accuracy\"]:  # \"Accuracy\", \"AOPC+\", \"LOdds\", \"AOPC\"\n",
    "        plot_main_fig(METRIC, filter_dataset, LEGEND_CONFIG_MAP, save_fig=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf4a335",
   "metadata": {},
   "source": [
    "## 4) Roberta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d6600b",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEGEND_CONFIG_MAP = {\n",
    "    \"classifier Decomposition AbsDot Bias\": \"DecompX\",\n",
    "#     \"classifier +Decomposition AbsDot Bias\": \"+DecompX\",\n",
    "    \"aggregated IG_ALL_ZERO_NORM_prediction_based\": \"Integrated Gradient\",\n",
    "    \"aggregated IXG_NORM_prediction_based\": \"GradientInput\",\n",
    "    \"aggregated GlobEnc\": \"GlobEnc\",\n",
    "    \"aggregated GlobEnc No Bias FFN\": \"GlobEnc FFN\",\n",
    "    \"aggregated ALTI\": \"ALTI\",\n",
    "}\n",
    "for filter_dataset in ['mnli']:  # 'sst2', 'mnli', 'qnli', 'hatexplain'\n",
    "    for METRIC in [\"AOPC\", \"Accuracy\"]:  # \"Accuracy\", \"AOPC+\", \"LOdds\", \"AOPC\"\n",
    "        plot_main_fig(METRIC, filter_dataset, LEGEND_CONFIG_MAP, model=\"roberta\", save_fig=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f2b792",
   "metadata": {},
   "source": [
    "# Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7cd0b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807cde15",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "globencs = dict()\n",
    "for model_dataset_set in tqdm(MODEL_DATASET_SET, desc=\"Models_Dataset_Sets\"):\n",
    "    model_checkpoint, task_name, set_of_data = model_dataset_set\n",
    "    for globenc_cfg_name in tqdm(GLOBENC_CONFIG_NAMES, desc=\"Globenc Configs\"):\n",
    "        file_name = f\"[{task_name}]_[{set_of_data}]_[{'-'.join(model_checkpoint.split('/')[-2:])}]_[{globenc_cfg_name}]\"\n",
    "        \n",
    "        df = pd.read_pickle(f\"{OUTPUT_DIR}/{file_name}.pkl\")\n",
    "        globencs[f\"{globenc_cfg_name}\"] = df\n",
    "        \n",
    "        print(file_name)\n",
    "        idx = 10\n",
    "        print_globenc(\n",
    "            df[\"globenc_last_layer\"].iloc[idx], \n",
    "            df[\"tokens\"].iloc[idx], \n",
    "            prefix=f\"{globenc_cfg_name}:\".ljust(13), \n",
    "            no_cls_sep=True,\n",
    "            discrete=False\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e1670e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ratio = 0.8\n",
    "index_to_value_d = {\n",
    "    \"masking_ratios\": ratio, \n",
    "    \"model_checkpoint\": model_checkpoint, \n",
    "    \"task_name\": task_name, \n",
    "    \"set_of_data\": set_of_data, \n",
    "    \"globenc_config_name\": \"decomp\",\n",
    "}\n",
    "\n",
    "index_to_value_g = {\n",
    "    \"masking_ratios\": ratio, \n",
    "    \"model_checkpoint\": model_checkpoint, \n",
    "    \"task_name\": task_name, \n",
    "    \"set_of_data\": set_of_data, \n",
    "    \"globenc_config_name\": \"globenc\",\n",
    "}\n",
    "\n",
    "df_d = results.get(index_to_value_d)\n",
    "df_g = results.get(index_to_value_g)\n",
    "idxs = np.linalg.norm(\n",
    "    scipy.special.softmax(np.array(df_d[\"logits\"].tolist()), axis=-1) - \n",
    "    scipy.special.softmax(np.array(df_g[\"logits\"].tolist()), axis=-1)\n",
    "    , axis=-1\n",
    ").argsort()\n",
    "# idxs = np.linalg.norm(np.array(df_d[\"logits\"].tolist()) - np.array(df_g[\"logits\"].tolist()), axis=-1).argsort()\n",
    "idxs = np.flip(idxs)\n",
    "idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ccd5050",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for idx in idxs[0:20]:\n",
    "    print(idx)\n",
    "    for cfg in [\"decomp\", \"globenc\", \"globenc_ffn\"]:\n",
    "        print_globenc(\n",
    "            globencs[cfg][\"globenc_last_layer\"].iloc[idx], \n",
    "            globencs[cfg][\"tokens\"].iloc[idx], \n",
    "            prefix=f\"{cfg}:\".ljust(13), \n",
    "            no_cls_sep=True,\n",
    "            discrete=True\n",
    "        )\n",
    "    with pd.option_context('display.max_colwidth', None):\n",
    "        print(\"decomp:\")\n",
    "        display(df_d.iloc[[idx]])\n",
    "        print(\"globenc:\")\n",
    "        display(df_g.iloc[[idx]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ceedd93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "itertools.product(*results.indices.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b87ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = results.get({'masking_ratios': 0.0, 'model_checkpoint': '/home/modaresi/projects/globenc_analysis/outputs/models/output_sst2_bert-base-uncased_0001_SEED0042/checkpoint-10525', 'task_name': 'sst2', 'set_of_data': 'validation', 'globenc_config_name': 'decomp'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff4ed4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in sorted(results.indices['masking_ratios']):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3aa41d3",
   "metadata": {},
   "source": [
    "# IG to Pandas Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc1be74",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sal_path = \"/home/modaresi/projects/globenc_analysis/outputs/integrated_saliencies/\"\n",
    "sal_path = \"/home/modaresi/projects/globenc_analysis/outputs/saliencies/\"\n",
    "\n",
    "dataset_map = {\n",
    "#     \"cola\": \"/home/modaresi/projects/globenc_analysis/outputs/globencs_v3/[cola]_[validation]_[output_cola_bert-base-uncased_0001_SEED0042-checkpoint-1340]_[Decomposition LinearApproximation].pkl\",\n",
    "#     \"qnli\": \"/home/modaresi/projects/globenc_analysis/outputs/globencs_v3/[qnli]_[validation]_[output_qnli_bert-base-uncased_0001_SEED0042-checkpoint-16370]_[Decomposition LinearApproximation].pkl\",\n",
    "#     \"mnli\": \"/home/modaresi/projects/globenc_analysis/outputs/globencs_v3/[mnli]_[validation_matched]_[output_mnli_bert-base-uncased_0001_SEED0042-checkpoint-61360]_[Decomposition ZeroOriginApproximation].pkl\",\n",
    "#     \"sst2\": \"/home/modaresi/projects/globenc_analysis/outputs/globencs_v3/[sst2]_[validation]_[output_sst2_bert-base-uncased_0001_SEED0042-checkpoint-10525]_[GlobEnc FFN LinearApproximation].pkl\",\n",
    "#     \"hatexplain\": \"/home/modaresi/projects/globenc_analysis/outputs/globencs_v3/[hatexplain]_[validation]_[output_hatexplain_bert-base-uncased_0001_SEED0042-checkpoint-2405]_[GlobEnc].pkl\",\n",
    "    \"mnli\": \"/home/modaresi/projects/globenc_analysis/outputs/globencs_v3/[mnli]_[validation_matched]_[WillHeld-roberta-base-mnli]_[Decomposition AbsDot Bias].pkl\",\n",
    "    \"sst2\": \"/home/modaresi/projects/globenc_analysis/outputs/globencs_v3/[sst2]_[validation]_[WillHeld-roberta-base-sst2]_[Decomposition AbsDot Bias].pkl\",\n",
    "}\n",
    "\n",
    "for file_name in tqdm(os.listdir(sal_path)):\n",
    "    dataset_df = None\n",
    "    for ds, path in dataset_map.items():\n",
    "        if ds in file_name:\n",
    "            dataset_df = pd.read_pickle(path)[[\"tokens\", \"logits\", \"label\"]]\n",
    "            lengths = dataset_df[\"tokens\"].str.len()\n",
    "    if dataset_df is None or not \"roberta\" in file_name:\n",
    "        continue\n",
    "    if \".npy\" in file_name:\n",
    "        a = np.load(f\"{sal_path}{file_name}\")\n",
    "        importance = [np.expand_dims(a_i[:lengths[i]], axis=0) for i, a_i in enumerate(a)]\n",
    "        df = pd.DataFrame({\n",
    "            \"importance_last_layer_aggregated\": importance,\n",
    "            \"tokens\": dataset_df[\"tokens\"].values,\n",
    "            \"logits\": dataset_df[\"logits\"].values,\n",
    "            \"label\": dataset_df[\"label\"].values,\n",
    "        })\n",
    "        f = f\"{sal_path}{file_name.split('.')[0]}.pkl\"\n",
    "        print(f)\n",
    "        df.to_pickle(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ccebcb2",
   "metadata": {},
   "source": [
    "# ALTI to Pandas Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6de2b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# alti_path = \"/home/modaresi/projects/globenc_analysis/notebooks/ALTI/transformer-contributions/data/validation/\"\n",
    "alti_path = \"/home/modaresi/projects/globenc_analysis/temp/\"\n",
    "ALTI_OUTPUT_DIR = \"/home/modaresi/projects/globenc_analysis/outputs/ALTI\"\n",
    "\n",
    "dataset_map = {\n",
    "#     \"cola\": \"/home/modaresi/projects/globenc_analysis/outputs/globencs_v3/[cola]_[validation]_[output_cola_bert-base-uncased_0001_SEED0042-checkpoint-1340]_[Decomposition AbsDot Bias].pkl\",\n",
    "#     \"qnli\": \"/home/modaresi/projects/globenc_analysis/outputs/globencs_v3/[qnli]_[validation]_[output_qnli_bert-base-uncased_0001_SEED0042-checkpoint-16370]_[Decomposition AbsDot Bias].pkl\",\n",
    "#     \"mnli\": \"/home/modaresi/projects/globenc_analysis/outputs/globencs_v3/[mnli]_[validation_matched]_[output_mnli_bert-base-uncased_0001_SEED0042-checkpoint-61360]_[Decomposition AbsDot Bias].pkl\",\n",
    "#     \"sst2\": \"/home/modaresi/projects/globenc_analysis/outputs/globencs_v3/[sst2]_[validation]_[output_sst2_bert-base-uncased_0001_SEED0042-checkpoint-10525]_[Decomposition AbsDot Bias].pkl\",\n",
    "#     \"hatexplain\": \"/home/modaresi/projects/globenc_analysis/outputs/globencs_v3/[hatexplain]_[validation]_[output_hatexplain_bert-base-uncased_0001_SEED0042-checkpoint-2405]_[Decomposition AbsDot Bias].pkl\",\n",
    "    \"mnli\": \"/home/modaresi/projects/globenc_analysis/outputs/globencs_v3/[mnli]_[validation_matched]_[WillHeld-roberta-base-mnli]_[Decomposition AbsDot Bias].pkl\",\n",
    "    \"sst2\": \"/home/modaresi/projects/globenc_analysis/outputs/globencs_v3/[sst2]_[validation]_[WillHeld-roberta-base-sst2]_[Decomposition AbsDot Bias].pkl\",\n",
    "}\n",
    "\n",
    "for file_name in tqdm(os.listdir(alti_path)):\n",
    "    for ds, path in dataset_map.items():\n",
    "        if ds in file_name:\n",
    "            dataset_df = pd.read_pickle(path)[[\"tokens\", \"logits\", \"label\"]]\n",
    "            lengths = dataset_df[\"tokens\"].str.len()\n",
    "    if \".npy\" in file_name:\n",
    "        a = np.load(f\"{alti_path}{file_name}\", allow_pickle=True).item()\n",
    "        preds = a[\"pred_class\"]\n",
    "        a = a[\"ours\"]\n",
    "        importance = [np.expand_dims(a_i, axis=0) for i, a_i in enumerate(a)]\n",
    "        df = pd.DataFrame({\n",
    "            \"importance_last_layer_aggregated\": importance,\n",
    "            \"tokens\": dataset_df[\"tokens\"].values,\n",
    "            \"logits\": dataset_df[\"logits\"].values,\n",
    "            \"label\": dataset_df[\"label\"].values,\n",
    "            \"pred_class\": preds,\n",
    "        })\n",
    "        fn = make_result_filename(dataset_df.attrs[\"task_name\"], dataset_df.attrs[\"set_of_data\"], dataset_df.attrs[\"model_checkpoint\"], \"ALTI\")\n",
    "        f = f\"{ALTI_OUTPUT_DIR}/{fn}.pkl\"\n",
    "        print(f)\n",
    "        df.to_pickle(f)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a7c349",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df.attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9fa0bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 50\n",
    "print(len(df.iloc[idx][\"tokens\"]))\n",
    "print(len(df.iloc[idx][\"importance_last_layer_aggregated\"][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82c0c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.load(f\"/home/modaresi/projects/globenc_analysis/outputs/integrated_saliencies/[qnli]_[validation]_[output_qnli_bert-base-uncased_0001_SEED0042-checkpoint-16370]_[IG_ALL_PAD_SUM_label_based].npy\")\n",
    "b = np.load(f\"/home/modaresi/projects/globenc_analysis/outputs/integrated_saliencies/[qnli]_[validation]_[output_qnli_bert-base-uncased_0001_SEED0042-checkpoint-16370]_[ALL_PAD]_[SUM].npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5106ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a = np.load(f\"{path}bert_sst2_attributions.npy\", allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22101671",
   "metadata": {},
   "source": [
    "# Create DecompX+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea5f194",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dir_model_dataset_set in tqdm(DIR_MODEL_DATASET_SET, desc=\"Models_Dataset_Sets\"):\n",
    "    dir, model_checkpoint, task_name, set_of_data = dir_model_dataset_set\n",
    "    for cfg_name in tqdm(GLOBENC_CONFIG_NAMES, desc=\"Configs\"):\n",
    "        file_name = make_result_filename(task_name, set_of_data, model_checkpoint, cfg_name)\n",
    "        try:\n",
    "            df = pd.read_pickle(f\"{dir}/{file_name}.pkl\")\n",
    "            df[\"importance_last_layer_classifier\"] = df[\"importance_last_layer_classifier\"].apply(lambda x: np.abs(x))\n",
    "            file_name = make_result_filename(task_name, set_of_data, model_checkpoint, f\"+{cfg_name}\")\n",
    "            print(file_name)\n",
    "            df.to_pickle(f\"{dir}/{file_name}.pkl\")\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73c8fcf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[\"importance_last_layer_classifier\"] = df[\"importance_last_layer_classifier\"].apply(lambda x: np.abs(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffde2634",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"importance_last_layer_classifier\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2ef4b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
